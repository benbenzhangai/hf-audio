{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bba1ab9-6910-4c35-8159-8838d8a61e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for PolyAI/minds14 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/PolyAI/minds14\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Audio\n",
    "\n",
    "minds = load_dataset(\"PolyAI/minds14\", name=\"en-AU\", split=\"train\")\n",
    "minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bf9356-f1ed-4314-870a-3c6376089313",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3344306-75ad-4785-b504-11409568d12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052ec9006bc240de968ce0361335680b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b396b41f477c4c1c859b18de592c625f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2a30e446ca461388c749e5f69a4882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b516f50ab10465fab7e3e7fff0ea2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/212 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"audio-classification\",\n",
    "    model=\"anton-l/xtreme_s_xlsr_300m_minds14\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3309ab9d-4d0f-4a7a-a92c-801a426c98ae",
   "metadata": {},
   "source": [
    "This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the GOOGLE/XTREME_S - MINDS14.ALL dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0056c284-10a0-49cc-abdf-25ed9b1ce6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = minds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6df177e8-6340-43df-8942-7356b8319946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/teamspace/studios/this_studio/.cache/huggingface/datasets/downloads/extracted/02889adf80bc6103bb58df021e360f1b594319f40059d87953dcab9c5d45c2bf/en-AU~PAY_BILL/response_4.wav',\n",
       " 'audio': {'path': '/teamspace/studios/this_studio/.cache/huggingface/datasets/downloads/extracted/02889adf80bc6103bb58df021e360f1b594319f40059d87953dcab9c5d45c2bf/en-AU~PAY_BILL/response_4.wav',\n",
       "  'array': array([2.36119668e-05, 1.92324660e-04, 2.19284790e-04, ...,\n",
       "         9.40907281e-04, 1.16613181e-03, 7.20883254e-04]),\n",
       "  'sampling_rate': 16000},\n",
       " 'transcription': 'I would like to pay my electricity bill using my card can you please assist',\n",
       " 'english_transcription': 'I would like to pay my electricity bill using my card can you please assist',\n",
       " 'intent_class': 13,\n",
       " 'lang_id': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b1dd387-b899-4894-af2a-8aa4799c0ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9625310301780701, 'label': 'pay_bill'},\n",
       " {'score': 0.028672808781266212, 'label': 'freeze'},\n",
       " {'score': 0.0033498003613203764, 'label': 'card_issues'},\n",
       " {'score': 0.002005805494263768, 'label': 'abroad'},\n",
       " {'score': 0.0008484331192448735, 'label': 'high_value_payment'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(example[\"audio\"][\"array\"])[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d1f50d5-4664-473c-bf59-802579bd448a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pay_bill'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = minds.features[\"intent_class\"].int2str\n",
    "id2label(example[\"intent_class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6e7570-1d24-4126-ae93-0c67145b6777",
   "metadata": {},
   "source": [
    "## Automatic speech recognition (ASR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61d66cda-ad9f-4a40-a18c-e365df2ac17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/wav2vec2-base-960h and revision 22aad52 (https://huggingface.co/facebook/wav2vec2-base-960h).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1586cd01ad47558ee78719321e0cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622203479abf428f825c4e891bfdf0ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154dc5efa8c841b1a82eff283dcfef53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aeda90f461c478db312af1ea2475666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558eed1b94174862ae04ebdb1ba42f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd38ad47cd594060b11fbc3d3c43af98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "asr = pipeline(\"automatic-speech-recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75ae1e2a-7f02-4550-97cd-3a2d1a3a98fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I WOULD LIKE TO PAY MY ELECTRICITY BILL USING MY CAD CAN YOU PLEASE ASSIST'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = minds[0]\n",
    "asr(example[\"audio\"][\"array\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc642be3-567e-41f0-b7e7-8c12a011ebbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I would like to pay my electricity bill using my card can you please assist'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"english_transcription\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eefce6-c278-4e9b-b0bb-51d8c1d6ccbf",
   "metadata": {},
   "source": [
    "**Evaluation metrics**\n",
    "\n",
    "Word Error Rate (WER) — most widely used\n",
    "Definition:\n",
    "WER = (S + D + I) / N\n",
    "Where:\n",
    "\n",
    "S = Substitutions (wrong word instead of correct one)\n",
    "\n",
    "D = Deletions (missed word)\n",
    "\n",
    "I = Insertions (extra word)\n",
    "\n",
    "N = Total number of words in reference\n",
    "\n",
    "Example:\n",
    "\n",
    "Reference: how are you doing\n",
    "\n",
    "Hypothesis: how do you doing\n",
    "→ S = 1 (are → do), D = 0, I = 0\n",
    "→ WER = 1 / 4 = 25%\n",
    "\n",
    "WER is lower-is-better. A perfect ASR system would have WER = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bef9b9e3-76be-4487-ad7a-cf36b943aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def wer(reference: str, hypothesis: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Word Error Rate (WER) between a reference and a hypothesis sentence.\n",
    "\n",
    "    Args:\n",
    "        reference (str): The ground truth sentence.\n",
    "        hypothesis (str): The ASR model output sentence.\n",
    "\n",
    "    Returns:\n",
    "        float: WER = (S + D + I) / N\n",
    "    \"\"\"\n",
    "    ref_words = reference.strip().split()\n",
    "    hyp_words = hypothesis.strip().split()\n",
    "    n = len(ref_words)\n",
    "\n",
    "    # Initialize the edit distance matrix\n",
    "    dp = np.zeros((len(ref_words) + 1, len(hyp_words) + 1), dtype=int)\n",
    "\n",
    "    for i in range(len(ref_words) + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len(hyp_words) + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, len(ref_words) + 1):\n",
    "        for j in range(1, len(hyp_words) + 1):\n",
    "            if ref_words[i - 1] == hyp_words[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                substitution = dp[i - 1][j - 1] + 1\n",
    "                insertion = dp[i][j - 1] + 1\n",
    "                deletion = dp[i - 1][j] + 1\n",
    "                dp[i][j] = min(substitution, insertion, deletion)\n",
    "\n",
    "    wer_score = dp[len(ref_words)][len(hyp_words)] / n\n",
    "    return wer_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49cb052a-ef1e-49c7-8e60-e5d939c3a476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06666666666666667"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer((example[\"english_transcription\"]).lower(),\n",
    "    (asr(example[\"audio\"][\"array\"])['text']).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c5ee1a-de82-4b00-b52e-93486930288a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
